{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06e10cb7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-10T21:27:19.968846Z",
     "iopub.status.busy": "2025-11-10T21:27:19.968459Z",
     "iopub.status.idle": "2025-11-10T21:27:22.283157Z",
     "shell.execute_reply": "2025-11-10T21:27:22.281783Z"
    },
    "papermill": {
     "duration": 2.32182,
     "end_time": "2025-11-10T21:27:22.285517",
     "exception": false,
     "start_time": "2025-11-10T21:27:19.963697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/hull-tactical-market-prediction/train.csv\n",
      "/kaggle/input/hull-tactical-market-prediction/test.csv\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/default_inference_server.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/default_gateway.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/__init__.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/templates.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/base_gateway.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/relay.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/kaggle_evaluation.proto\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/__init__.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/generated/kaggle_evaluation_pb2.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/generated/kaggle_evaluation_pb2_grpc.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/generated/__init__.py\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d964f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T21:27:22.295240Z",
     "iopub.status.busy": "2025-11-10T21:27:22.294479Z",
     "iopub.status.idle": "2025-11-10T21:28:08.589682Z",
     "shell.execute_reply": "2025-11-10T21:28:08.588043Z"
    },
    "papermill": {
     "duration": 46.301505,
     "end_time": "2025-11-10T21:28:08.591457",
     "exception": false,
     "start_time": "2025-11-10T21:27:22.289952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Professional Pipeline Initialization Started ---\n",
      "Warning: MOM1 or V1 not found in current DataFrame schema. Skipping MOM1_V1_Ratio.\n",
      "Starting Time-Series Cross-Validation...\n",
      "Fold 1 Validation Sharpe: -0.0193\n",
      "Fold 2 Validation Sharpe: 0.1094\n",
      "Fold 3 Validation Sharpe: 0.1713\n",
      "Fold 4 Validation Sharpe: 0.1084\n",
      "Fold 5 Validation Sharpe: 0.0309\n",
      "\n",
      "Average CV Sharpe: 0.0801\n",
      "\n",
      "Retraining final model on ALL data (Step 7)...\n",
      "--- Predictor Initialized Successfully ---\n",
      "Warning: MOM1 or V1 not found in current DataFrame schema. Skipping MOM1_V1_Ratio.\n",
      "Warning: MOM1 or V1 not found in current DataFrame schema. Skipping MOM1_V1_Ratio.\n",
      "Warning: MOM1 or V1 not found in current DataFrame schema. Skipping MOM1_V1_Ratio.\n",
      "Warning: MOM1 or V1 not found in current DataFrame schema. Skipping MOM1_V1_Ratio.\n",
      "Warning: MOM1 or V1 not found in current DataFrame schema. Skipping MOM1_V1_Ratio.\n",
      "Warning: MOM1 or V1 not found in current DataFrame schema. Skipping MOM1_V1_Ratio.\n",
      "Warning: MOM1 or V1 not found in current DataFrame schema. Skipping MOM1_V1_Ratio.\n",
      "Warning: MOM1 or V1 not found in current DataFrame schema. Skipping MOM1_V1_Ratio.\n",
      "Warning: MOM1 or V1 not found in current DataFrame schema. Skipping MOM1_V1_Ratio.\n",
      "Warning: MOM1 or V1 not found in current DataFrame schema. Skipping MOM1_V1_Ratio.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# IMPORTANT: The competition environment requires these imports for serving the model.\n",
    "import kaggle_evaluation.default_inference_server\n",
    "\n",
    "# --- Global Configuration & File Paths ---\n",
    "TRAIN_FILE = '/kaggle/input/hull-tactical-market-prediction/train.csv'\n",
    "DATE_ID_COL = 'date_id'\n",
    "TARGET_COL = 'market_forward_excess_returns'\n",
    "RISK_FREE_COL = 'risk_free_rate'\n",
    "LAGGED_RISK_FREE_COL = 'lagged_risk_free_rate'\n",
    "\n",
    "# --- Step 1: Sharpe Ratio Calculation (The Metric) ---\n",
    "\n",
    "def calculate_sharpe_ratio(df: pd.DataFrame, position_col: str = 'prediction') -> float:\n",
    "    \"\"\"\n",
    "    Calculates the competition's Sharpe Ratio. \n",
    "    Formula: Sharpe = (Mean(Portfolio Returns)) / (StdDev(Portfolio Returns))\n",
    "    \n",
    "    Portfolio Return = Position * (Forward Return - Risk Free Rate) + Risk Free Rate\n",
    "    where Position is clamped [0, 2].\n",
    "    \"\"\"\n",
    "    # Use 'forward_returns' and 'risk_free_rate' columns for solution calculation\n",
    "    daily_returns = (\n",
    "        df[position_col] * (df['forward_returns'] - df[RISK_FREE_COL]) + df[RISK_FREE_COL]\n",
    "    )\n",
    "    \n",
    "    # Competition uses a specific scaling (often T**0.5 for annualized Sharpe), \n",
    "    # but for local CV comparison, the daily Sharpe is sufficient and less noisy.\n",
    "    mean_return = daily_returns.mean()\n",
    "    std_dev = daily_returns.std()\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if std_dev == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    return mean_return / std_dev\n",
    "\n",
    "# --- Feature Engineering Class (Step 3) ---\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"\n",
    "    Encapsulates all feature creation logic to ensure consistency between\n",
    "    training and inference. Uses Polars for speed.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Base features to be used in the model\n",
    "        self.feature_cols = []\n",
    "        # Raw features list is now initialized empty and set in Predictor._train_and_initialize\n",
    "        self.raw_features = []\n",
    "    \n",
    "    def set_raw_features(self, df_columns: list):\n",
    "        \"\"\"Sets the list of available raw features based on the loaded Polars schema.\"\"\"\n",
    "        self.raw_features = [\n",
    "            col for col in df_columns\n",
    "            if col.startswith(('M', 'E', 'I', 'P', 'V', 'S', 'D', 'MOM')) or col.startswith('lagged')\n",
    "        ]\n",
    "        \n",
    "    def create_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Applies feature engineering to the Polars DataFrame.\"\"\"\n",
    "        \n",
    "        df_out = df.clone()\n",
    "        \n",
    "        # 1. Lagged Features (Step 3: Lagged Features)\n",
    "        # We need a buffer of at least 20 days for the rolling features below.\n",
    "        LAG_WINDOW = [1, 5]\n",
    "        for col in self.raw_features:\n",
    "            for lag in LAG_WINDOW:\n",
    "                df_out = df_out.with_columns(\n",
    "                    pl.col(col).shift(lag).alias(f'{col}_lag_{lag}')\n",
    "                )\n",
    "\n",
    "        # 2. Rolling Statistics (Step 3: Rolling Statistics)\n",
    "        # Expanded feature coverage for rolling statistics\n",
    "        for col in ['V1', 'M1', 'E1', 'S1', 'D1']: \n",
    "            if col in df_out.columns:\n",
    "                for window in [5, 20]:\n",
    "                    # Use Polars' rolling functions. Since this runs on the history buffer,\n",
    "                    # the window naturally only looks backwards relative to the last row.\n",
    "                    df_out = df_out.with_columns(\n",
    "                        pl.col(col).rolling_mean(window_size=window).alias(f'{col}_roll_mean_{window}'),\n",
    "                        pl.col(col).rolling_std(window_size=window).alias(f'{col}_roll_std_{window}')\n",
    "                    )\n",
    "        \n",
    "        # New: Add a simple ratio feature (MOM/VIX proxy, good for capturing risk-adjusted momentum)\n",
    "        # FIX: Added a check for column existence to prevent ColumnNotFoundError\n",
    "        if 'MOM1' in df_out.columns and 'V1' in df_out.columns:\n",
    "            df_out = df_out.with_columns(\n",
    "                (pl.col('MOM1') / pl.col('V1').fill_null(1e-6)).alias('MOM1_V1_Ratio')\n",
    "            )\n",
    "        else:\n",
    "             print(\"Warning: MOM1 or V1 not found in current DataFrame schema. Skipping MOM1_V1_Ratio.\")\n",
    "        \n",
    "        # 3. Differencing (Step 3: Differencing)\n",
    "        for col in ['P1', 'I1']:\n",
    "            if col in df_out.columns:\n",
    "                df_out = df_out.with_columns(\n",
    "                    (pl.col(col) - pl.col(col).shift(1)).alias(f'{col}_diff_1')\n",
    "                )\n",
    "\n",
    "        # Final list of features (excluding NaNs and original non-lagged features)\n",
    "        self.feature_cols = [\n",
    "            col for col in df_out.columns \n",
    "            if not col in (self.raw_features + [DATE_ID_COL, TARGET_COL, 'forward_returns', RISK_FREE_COL]) \n",
    "            and not col.startswith('lagged')\n",
    "        ]\n",
    "        \n",
    "        return df_out\n",
    "\n",
    "# --- Predictor Class (Step 6: Architecture & State Management) ---\n",
    "\n",
    "class Predictor:\n",
    "    \"\"\"\n",
    "    Manages the professional workflow: model, feature engineering, and state\n",
    "    (historical data) for sequential inference in a Code Competition.\n",
    "    \"\"\"\n",
    "    def __init__(self, history_size=60):\n",
    "        self.fe = FeatureEngineer()\n",
    "        self.model = None\n",
    "        self.is_initialized = False\n",
    "        self.HISTORY_SIZE = history_size\n",
    "        \n",
    "        # Buffer to store historical data needed for rolling/lagged features\n",
    "        self.history_buffer = pl.DataFrame()\n",
    "        # NEW: List of columns that must be Float64 for consistency\n",
    "        self.float_cols_in_train = [] \n",
    "\n",
    "        # 1. DEFINE MASTER COLUMNS FOR HISTORY BUFFER \n",
    "        # This list defines ALL columns we *want* in our buffer schema for consistency\n",
    "        self.history_master_cols = [\n",
    "            DATE_ID_COL, \n",
    "            RISK_FREE_COL, \n",
    "            'forward_returns', \n",
    "            LAGGED_RISK_FREE_COL, # Included here for schema consistency during concat\n",
    "            TARGET_COL \n",
    "        ] # We will append self.fe.raw_features later after loading the data\n",
    "        \n",
    "        self._train_and_initialize()\n",
    "\n",
    "    def _impute_and_preprocess(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Handles initial data cleaning and time-series compliant imputation.\n",
    "        (Step 1: Handle Missing Data)\n",
    "        \"\"\"\n",
    "        # Forward fill (ffill) any NaNs, carrying the last known value forward\n",
    "        df = df.ffill()\n",
    "        # Back fill (bfill) any remaining NaNs (usually early data) with the next available data\n",
    "        df = df.bfill()\n",
    "        \n",
    "        # Final safety for any remaining (e.g., if the whole series is NaN)\n",
    "        df = df.fillna(0) \n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _backtest_and_cv(self, df: pd.DataFrame, model_params: dict):\n",
    "        \"\"\"\n",
    "        Performs robust time-series cross-validation (Step 2).\n",
    "        Uses a LightGBM baseline (Step 4).\n",
    "        \"\"\"\n",
    "        print(\"Starting Time-Series Cross-Validation...\")\n",
    "        tscv = TimeSeriesSplit(n_splits=5, test_size=180) # 180 days is the size of the scored test set\n",
    "        \n",
    "        X = df[self.fe.feature_cols]\n",
    "        y = df[TARGET_COL]\n",
    "        \n",
    "        sharpe_scores = []\n",
    "        \n",
    "        # Initialize the model with the best parameters found (hypothetically)\n",
    "        model = lgb.LGBMRegressor(**model_params)\n",
    "        \n",
    "        for fold, (train_index, val_index) in enumerate(tscv.split(X)):\n",
    "            X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "            y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            val_preds = model.predict(X_val)\n",
    "            \n",
    "            # Post-process for validation to calculate Sharpe\n",
    "            val_df = df.iloc[val_index].copy()\n",
    "            val_df['predicted_excess_return'] = val_preds\n",
    "            \n",
    "            # Simplified volatility prediction for allocation (Step 5)\n",
    "            # In a real model, you'd train a separate volatility model.\n",
    "            val_df['predicted_volatility'] = val_df['predicted_excess_return'].rolling(window=20).std().shift(-1).fillna(0.005)\n",
    "\n",
    "            val_df['prediction'] = val_df.apply(\n",
    "                lambda row: self._calculate_position_allocation(\n",
    "                    row['predicted_excess_return'], \n",
    "                    row[RISK_FREE_COL], # Use RISK_FREE_COL for training/CV\n",
    "                    row['predicted_volatility']\n",
    "                ), axis=1\n",
    "            )\n",
    "            \n",
    "            # Calculate Sharpe Ratio on the validation fold (Step 2: Target the Metric)\n",
    "            sharpe = calculate_sharpe_ratio(val_df, position_col='prediction')\n",
    "            sharpe_scores.append(sharpe)\n",
    "            print(f\"Fold {fold+1} Validation Sharpe: {sharpe:.4f}\")\n",
    "\n",
    "        print(f\"\\nAverage CV Sharpe: {np.mean(sharpe_scores):.4f}\")\n",
    "        return model\n",
    "\n",
    "    def _train_and_initialize(self):\n",
    "        \"\"\"\n",
    "        Runs the full training pipeline (Steps 1, 3, 4) and sets the history buffer (Step 6).\n",
    "        \"\"\"\n",
    "        print(\"--- Professional Pipeline Initialization Started ---\")\n",
    "        \n",
    "        # 1. Load Data (Step 1)\n",
    "        df_raw = pl.read_csv(TRAIN_FILE)\n",
    "        \n",
    "        # NEW: Set raw features based on the loaded Polars DataFrame\n",
    "        self.fe.set_raw_features(df_raw.columns)\n",
    "\n",
    "        # Update master column list now that raw_features is set\n",
    "        self.history_master_cols.extend(self.fe.raw_features)\n",
    "        # Use set to remove duplicates and list() to ensure proper type for Polars filtering\n",
    "        self.history_master_cols = list(set(self.history_master_cols))\n",
    "        \n",
    "        # --- FIX 1/2: Store the list of columns that must be cast to Float64 ---\n",
    "        # Identify columns that should be float (raw features + targets/rates)\n",
    "        cols_to_check = self.fe.raw_features + [TARGET_COL, 'forward_returns', RISK_FREE_COL, LAGGED_RISK_FREE_COL]\n",
    "        \n",
    "        # Store the list of columns that were cast to Float64 in training\n",
    "        self.float_cols_in_train = [col for col in cols_to_check if col in df_raw.columns]\n",
    "        \n",
    "        df_raw = df_raw.with_columns([\n",
    "            pl.col(col).cast(pl.Float64, strict=False) for col in self.float_cols_in_train\n",
    "        ])\n",
    "        # -----------------------------------------------------------------------\n",
    "        \n",
    "        # 2. Seed History Buffer (Step 6: Global State Management)\n",
    "        \n",
    "        # Filter down the master column list to only include columns that exist in the training file (df_raw)\n",
    "        train_cols_for_seeding = [col for col in self.history_master_cols if col in df_raw.columns]\n",
    "\n",
    "        self.history_buffer = df_raw.tail(self.HISTORY_SIZE).select(train_cols_for_seeding)\n",
    "\n",
    "        # 3. Feature Engineering (Step 3) - Done on the full Polars data for training\n",
    "        df_features = self.fe.create_features(df_raw).to_pandas()\n",
    "        \n",
    "        # 4. Imputation (Step 1)\n",
    "        df_processed = self._impute_and_preprocess(df_features)\n",
    "        \n",
    "        # 5. Define Model Parameters (Step 4: Gradient Boosting)\n",
    "        lgbm_params = {\n",
    "            'objective': 'regression_l1',\n",
    "            'metric': 'mae',\n",
    "            'n_estimators': 500,\n",
    "            'learning_rate': 0.03,\n",
    "            'feature_fraction': 0.7,\n",
    "            'bagging_fraction': 0.7,\n",
    "            'bagging_freq': 1,\n",
    "            'verbose': -1,\n",
    "            'n_jobs': -1,\n",
    "            'seed': 42\n",
    "        }\n",
    "        \n",
    "        # 6. Cross-Validation and Final Retraining (Step 2 & Step 7)\n",
    "        # We run CV, then retrain on the whole data for the final model\n",
    "        self._backtest_and_cv(df_processed, lgbm_params)\n",
    "\n",
    "        print(\"\\nRetraining final model on ALL data (Step 7)...\")\n",
    "        # Final Model Training on ALL available data\n",
    "        X_all = df_processed[self.fe.feature_cols]\n",
    "        y_all = df_processed[TARGET_COL]\n",
    "        self.model = lgb.LGBMRegressor(**lgbm_params)\n",
    "        self.model.fit(X_all, y_all)\n",
    "        \n",
    "        self.is_initialized = True\n",
    "        print(\"--- Predictor Initialized Successfully ---\")\n",
    "\n",
    "    def _calculate_position_allocation(self, \n",
    "                                       predicted_excess_return: float, \n",
    "                                       risk_free_rate: float,\n",
    "                                       predicted_volatility: float = 0.005) -> float:\n",
    "        \"\"\"\n",
    "        Implements the Optimal Allocation Strategy (Step 5).\n",
    "        Transforms the predicted return into a constrained position size [0.0, 2.0].\n",
    "        \"\"\"\n",
    "        # --- Step 5: Optimal Allocation Strategy (Simplified Kelly-like) ---\n",
    "        \n",
    "        # K is a scaling factor (needs tuning). Increased to 150 for a more aggressive allocation.\n",
    "        K = 150 \n",
    "        \n",
    "        # Avoid division by near-zero volatility\n",
    "        vol = max(predicted_volatility, 1e-6)\n",
    "        \n",
    "        # Signal based on predicted return, scaled by predicted volatility\n",
    "        position = K * (predicted_excess_return / vol)\n",
    "        \n",
    "        # Mandatory clamping (Step 5: Clamping)\n",
    "        return np.clip(position, 0.0, 2.0).item()\n",
    "\n",
    "    def predict(self, test: pl.DataFrame) -> float:\n",
    "        \"\"\"\n",
    "        The required inference function (Step 6).\n",
    "        Runs sequentially for each day.\n",
    "        \"\"\"\n",
    "        # Ensure initialization has completed (runs only on day 1)\n",
    "        if not self.is_initialized:\n",
    "            # This should have run in __init__, but as a safety:\n",
    "            self._train_and_initialize() \n",
    "\n",
    "        # --- FIX 2/2: Cast incoming Polars DataFrame to Float64 for consistency ---\n",
    "        cols_to_cast_in_test = [col for col in self.float_cols_in_train if col in test.columns]\n",
    "        test = test.with_columns([\n",
    "            pl.col(col).cast(pl.Float64, strict=False) for col in cols_to_cast_in_test\n",
    "        ])\n",
    "        # -------------------------------------------------------------------------------------\n",
    "\n",
    "        master_cols = self.history_master_cols\n",
    "        \n",
    "        # 1. Prepare the 'test' row to match the schema of the history buffer\n",
    "        # The test DataFrame (one row) is padded to the full master schema.\n",
    "        test_row_data = {\n",
    "            col: test[col][0] if col in test.columns else None \n",
    "            for col in master_cols\n",
    "        }\n",
    "        \n",
    "        # Create a new DataFrame from the dictionary, resulting in the master column count\n",
    "        test_padded = pl.DataFrame([test_row_data])\n",
    "        \n",
    "        # Select only the columns that exist in the current history_buffer schema \n",
    "        cols_to_select_for_concat = self.history_buffer.columns\n",
    "        test_for_concat = test_padded.select(cols_to_select_for_concat)\n",
    "\n",
    "        # 2. Update Historical Buffer\n",
    "        self.history_buffer = pl.concat([self.history_buffer, test_for_concat]).tail(self.HISTORY_SIZE)\n",
    "        \n",
    "        # 3. Feature Engineering on updated buffer\n",
    "        # This calculates all rolling/lagged features correctly for the latest row (the 'test' day)\n",
    "        current_features_df = self.fe.create_features(self.history_buffer)\n",
    "        \n",
    "        # The data row to predict on is the *last* row of the features dataframe\n",
    "        X_predict = current_features_df.tail(1)[self.fe.feature_cols].to_pandas()\n",
    "        \n",
    "        # 4. Model Inference: Predict the excess return\n",
    "        try:\n",
    "            predicted_excess_return = self.model.predict(X_predict)[0]\n",
    "        except Exception as e:\n",
    "            # Safe fallback\n",
    "            print(f\"Prediction failed: {e}. Falling back to 0.0 allocation.\")\n",
    "            return 0.0\n",
    "\n",
    "        # 5. Post-Processing: Calculate the final 0.0 to 2.0 allocation\n",
    "        \n",
    "        # Use the lagged risk-free rate provided in the test set\n",
    "        risk_free_rate = test[LAGGED_RISK_FREE_COL][0]\n",
    "        \n",
    "        # Simple volatility estimate (replace with a model-based prediction for Step 5 refinement)\n",
    "        estimated_volatility = self.history_buffer['forward_returns'].std() if len(self.history_buffer) > 1 else 0.005\n",
    "        \n",
    "        final_allocation = self._calculate_position_allocation(\n",
    "            predicted_excess_return, \n",
    "            risk_free_rate,\n",
    "            estimated_volatility\n",
    "        )\n",
    "\n",
    "        return final_allocation\n",
    "\n",
    "# --- API Server Setup (Step 6 & 7) ---\n",
    "\n",
    "# Instantiate the predictor class. This triggers the entire training/CV pipeline.\n",
    "predictor = Predictor()\n",
    "\n",
    "# The final entry point function required by the competition\n",
    "# RENAMED to 'predict' to satisfy the gateway requirement\n",
    "def predict(test: pl.DataFrame) -> float:\n",
    "    return predictor.predict(test)\n",
    "\n",
    "# Setup the inference server\n",
    "# The server is instantiated with the standard 'predict' function name.\n",
    "inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n",
    "\n",
    "# Run the server based on the environment (local gateway for testing, serve for competition run)\n",
    "# Step 7: The competition requires running in this environment with Internet disabled.\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    # Local test mode (requires the data input path)\n",
    "    inference_server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14348714,
     "isSourceIdPinned": false,
     "sourceId": 111543,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 55.700549,
   "end_time": "2025-11-10T21:28:09.717364",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-10T21:27:14.016815",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
